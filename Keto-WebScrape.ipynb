{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib2\n",
    "import urllib\n",
    "import re\n",
    "import sys\n",
    "\n",
    "# sys.setdefaultencoding() does not exist, here!\n",
    "reload(sys)  # Reload does the trick!\n",
    "sys.setdefaultencoding('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Required Info\n",
    "0. webpage url\n",
    "1. image url [func: scrape_image_url()]\n",
    "2. entry title [func: scrape_entry_title()]\n",
    "3. detail url [func: scrape_detail_url()]\n",
    "4. ingredient (inside detail url) [func: scrape_ingredient()]\n",
    "5. next webpage url [func: scrape_next_url()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Scrape(object):\n",
    "    \"\"\"Web Scrape: ruled.me/keto-recipes\n",
    "    \n",
    "    Attributes:\n",
    "        url: A string with keto recipes\n",
    "        mealtype: A string with type of meal for classification\n",
    "       \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, url, mealtype):\n",
    "        \"\"\"Return a new Scrape object.\"\"\"\n",
    "        req = urllib2.Request(url, headers={'User-Agent' : \"Magic Browser\"}) \n",
    "        con = urllib2.urlopen( req )\n",
    "        output = con.read()\n",
    "        self.data = BeautifulSoup(output, \"lxml\")\n",
    "        self.url = url\n",
    "        self.mealtype = mealtype\n",
    "        self.post = self.data.findAll(\"div\", { \"class\" : \"type-post\" })\n",
    "\n",
    "    # 1.image url [func: scrape_image_url()]\n",
    "    def scrape_image_url(self):\n",
    "        \"\"\" Return the url for all the images\"\"\"\n",
    "        self.image_url = [i.find('img')['src'] for i in self.post]\n",
    "    \n",
    "    def save_image(self, location = 'pic/'):\n",
    "        \"\"\" Save image as a file\"\"\"\n",
    "        if location is None:\n",
    "            location = ''\n",
    "        \n",
    "        file_name = [location + i + '.jpg' for i in self.title]\n",
    "        for n in range(len(file_name)):\n",
    "            # print file_name[n]\n",
    "            urllib.urlretrieve(self.image_url[n], file_name[n])\n",
    "\n",
    "    # 2.entry title [func: scrape_entry_title()]\n",
    "    def scrape_entry_title(self):\n",
    "        \"\"\" Return the title for all the recipes\"\"\"\n",
    "        self.title = [i.find('h2').getText() for i in self.post]\n",
    "    \n",
    "    # 3.detail url [func: scrape_detail_url()]\n",
    "    def scrape_detail_url(self):\n",
    "        \"\"\" Return the url for detail information\"\"\"\n",
    "        self.detail_url = [i.find(\"div\", { \"class\" : \"post-data\" }).find('a')['href'] for i in self.post]\n",
    "\n",
    "    # 4.ingredient (inside detail url) [func: scrape_ingredient()]    \n",
    "    def scrape_ingredient(self):\n",
    "        \"\"\" Return ingredients from detail url \"\"\"\n",
    "        def ingredients(string_url):\n",
    "            # print string_url\n",
    "            \n",
    "            req = urllib2.Request(string_url, headers={'User-Agent' : \"Magic Browser\"}) \n",
    "            con = urllib2.urlopen( req )\n",
    "            output = con.read()\n",
    "            data = BeautifulSoup(output, \"lxml\")\n",
    "            try:\n",
    "                li_data = data.find(\"div\", { \"class\" : \"entry-content\" }).find('ul').findAll('li')\n",
    "                result = [i.getText() for i in li_data]\n",
    "            except:\n",
    "                result = []\n",
    "        \n",
    "            return result\n",
    "    \n",
    "        self.ingredient = [ingredients(i) for i in self.detail_url]\n",
    "    \n",
    "    # 5.next webpage url [func: scrape_next_url()]\n",
    "    def scrape_next_url(self, n=20):\n",
    "        \"\"\" Return the next url page for recipes \"\"\"\n",
    "        self.next_url = [self.url + 'page/' + str(i) for i in range(n) if i > 1]\n",
    "    \n",
    "    def scrape_all(self):\n",
    "        \"\"\" Return all the scraping done in above functions \"\"\"\n",
    "        self.scrape_image_url()\n",
    "        self.scrape_entry_title()\n",
    "        self.scrape_detail_url()\n",
    "        self.scrape_next_url()\n",
    "        self.scrape_ingredient()\n",
    "        self.save_image()\n",
    "        \n",
    "    def as_df(self):\n",
    "        \"\"\" Convert data as pandas DataFrame \"\"\"\n",
    "        import pandas as pd\n",
    "        \n",
    "        df = pd.DataFrame()\n",
    "        df['MealType'] = [self.mealtype] * len(self.title)\n",
    "        df['Title'] = self.title\n",
    "        df['ImageUrl'] = self.image_url\n",
    "        df['Ingredient'] = [' // '.join(i) for i in self.ingredient]\n",
    "        df['DetailUrl'] = self.detail_url\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def roll_through(self):\n",
    "        \"\"\" Repeat above process with next_url \"\"\"\n",
    "        import pandas as pd\n",
    "        self.scrape_all()\n",
    "        output_df = self.as_df()\n",
    "\n",
    "        for n_url in self.next_url:\n",
    "            try:\n",
    "                scrape_object = Scrape(n_url, self.mealtype)\n",
    "                scrape_object.scrape_all()\n",
    "                output_df = output_df.append(scrape_object.as_df())\n",
    "                # print scrape_object.as_df()\n",
    "            except:\n",
    "                print n_url\n",
    "                pass\n",
    "\n",
    "        file_name = \"Keto-Recipe-\" + self.mealtype + \".csv\"\n",
    "        print file_name\n",
    "        \n",
    "        output_df.to_csv(file_name,sep=\",\")\n",
    "        # return output_df\n",
    "            \n",
    "            \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_urls = [['breakfast', 'http://www.ruled.me/keto-recipes/breakfast/'],\n",
    "            ['lunch', 'http://www.ruled.me/keto-recipes/lunch/'],\n",
    "            ['dinner', 'http://www.ruled.me/keto-recipes/dinner/'],\n",
    "            ['dessert', 'http://www.ruled.me/keto-recipes/dessert/'],\n",
    "            ['snacks', 'http://www.ruled.me/keto-recipes/snacks/'],\n",
    "            ['side items', 'http://www.ruled.me/keto-recipes/side-items/'],\n",
    "            ['condiments', 'http://www.ruled.me/keto-recipes/condiments/']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "breakfast = Scrape('http://www.ruled.me/keto-recipes/breakfast/', 'breakfast')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for obj in all_urls:\n",
    "    scrape_object = Scrape(obj[1],obj[0])\n",
    "    scrape_object.roll_through()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "breakfast.scrape_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
